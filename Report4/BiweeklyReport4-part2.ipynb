{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464398e4-c94a-44c2-bada-d28471c026c8",
   "metadata": {},
   "source": [
    "# Implementing A Decision Tree #\n",
    "\n",
    "In this small notebook, I will try to implement a decision tree from scratch (no XGBoost) on the dataset mentioned in Part 1, to compare accuracy in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e83f820-5948-49dd-89dc-1b073c732c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for depth 4: 89.93%\n",
      "Accuracy for depth 5: 96.41%\n",
      "Accuracy for depth 6: 100.00%\n",
      "Accuracy for depth 7: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Used for impurity measure\n",
    "def Entropy(y):\n",
    "    class_counts = np.bincount(y)\n",
    "    total = len(y)\n",
    "    probabilities = class_counts / total\n",
    "    return -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
    "\n",
    "def Information_Gain(X,y,feature_index):\n",
    "    original_entropy = Entropy(y)\n",
    "    \n",
    "    feature_values = np.unique(X[:, feature_index])\n",
    "    \n",
    "    weighted_entropy = 0\n",
    "    for value in feature_values:\n",
    "        mask = X[:, feature_index] == value\n",
    "        y_subset = y[mask]\n",
    "        \n",
    "        # Calculate the weighted entropy \n",
    "        weighted_entropy += (len(y_subset) / len(y)) * Entropy(y_subset)\n",
    "    \n",
    "    # Information Gain is the reduction in entropy\n",
    "    return original_entropy - weighted_entropy\n",
    "\n",
    "def Cut(X,y,feature_index,value):\n",
    "    mask = X[:, feature_index] == value\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def Best_Cut(X,y):\n",
    "    best_info_gain = -float('inf')\n",
    "    best_split_info = None\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Iterate over all features\n",
    "    for feature_index in range(n_features):\n",
    "        # Calculate Information Gain for each feature\n",
    "        info_gain = Information_Gain(X, y, feature_index)\n",
    "        \n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain = info_gain\n",
    "            best_split_info = feature_index\n",
    "    \n",
    "    return best_split_info\n",
    "\n",
    "# Recursive function to build the tree (For this part I did use ChatGPT)\n",
    "def build_tree(X, y, depth=0, max_depth=5):\n",
    "    # Stopping conditions\n",
    "    if len(np.unique(y)) == 1:  # All samples are the same class\n",
    "        return {'label': np.unique(y)[0]}\n",
    "    if depth >= max_depth:  # Max depth reached\n",
    "        return {'label': np.bincount(y).argmax()}  # Majority class\n",
    "    \n",
    "    # Find the best split (highest Information Gain)\n",
    "    best_feature_index = Best_Cut(X, y)\n",
    "    best_feature_values = np.unique(X[:, best_feature_index])\n",
    "    \n",
    "    tree = {'feature_index': best_feature_index, 'children': {}}\n",
    "    \n",
    "    # Split data and create child nodes\n",
    "    for value in best_feature_values:\n",
    "        X_left, y_left = Cut(X, y, best_feature_index, value)\n",
    "        \n",
    "        # Recursively build the tree for each subset\n",
    "        tree['children'][value] = build_tree(X_left, y_left, depth + 1, max_depth)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict(tree, X):\n",
    "    if 'label' in tree:  \n",
    "        return tree['label']\n",
    "    \n",
    "  \n",
    "    feature_value = X[tree['feature_index']]\n",
    "    if feature_value in tree['children']:\n",
    "        return predict(tree['children'][feature_value], X)\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "def accuracy(tree, X, y):\n",
    "    predictions = [predict(tree, x) for x in X]\n",
    "    return np.mean(predictions == y)\n",
    "\n",
    "car_data = pd.read_csv(\"car.data\", header=None)\n",
    "columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'evaluation']\n",
    "car_data.columns = columns\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply encoding to all columns, including target 'evaluation'\n",
    "for column in car_data.columns:\n",
    "    car_data[column] = label_encoder.fit_transform(car_data[column])\n",
    "\n",
    "X = car_data.drop('evaluation',axis=1).values\n",
    "y = car_data['evaluation'].values\n",
    "\n",
    "tree_1 = build_tree(X, y, max_depth=4)\n",
    "tree_2 = build_tree(X, y, max_depth=5)\n",
    "tree_3 = build_tree(X, y, max_depth=6)\n",
    "tree_4 = build_tree(X, y, max_depth=7)\n",
    "\n",
    "acc_1 = accuracy(tree_1, X, y)\n",
    "acc_2 = accuracy(tree_2, X, y)\n",
    "acc_3 = accuracy(tree_3, X, y)\n",
    "acc_4 = accuracy(tree_4, X, y)\n",
    "\n",
    "print(f\"Accuracy for depth 4: {acc_1 * 100:.2f}%\")\n",
    "print(f\"Accuracy for depth 5: {acc_2 * 100:.2f}%\")\n",
    "print(f\"Accuracy for depth 6: {acc_3 * 100:.2f}%\")\n",
    "print(f\"Accuracy for depth 7: {acc_4 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15569df1-f32d-4cac-a71c-db9f4c61e1b5",
   "metadata": {},
   "source": [
    "# Summary #\n",
    "Comparing this to the previous notebook, it might be suprising, but there are a few things to consider:\n",
    "- While we achieved 100% at depth 6, thats a huge issue as we are now overfitting, and this model will most likely fail in production\n",
    "- The model in notebook 1 has a random_state parameter, used to improve effiecency.\n",
    "- While this model does have a good accuracy, this is over test data, and is never the end all be all. It could be that this model perfomrs well now, but cannot generalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf3122-9131-4e99-a8df-e60625e9418b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
