{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21ef43b-9fca-48c8-8d5d-c5c19b814db8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Biweekly Report 2: Adam Optimization vs The Rest #\n",
    "For this biweekly report, I take a delve into one of the most cited, most used loss-minimization algorithms, such that it has become the default in many libraries today. I compare Adam against other minimzation technqiues, as well as attempt to breakdown and understand it at it's barebones. I would like to preface that for my testing/figures, a CNN is used for classifcation. We have not yet covered such structures, and while I do have experience with them, I mainly wanted to focus on having a complete understanding of these optimizers, which I believe I confidently can say I had.\n",
    "\n",
    "# Where to start ? Answer: SGD #\n",
    "Well in order to explain Adam, there are a few prerequisites. For example, what exactly is an optimization technique, and what are we even optimizing ? To answer that, let us introduce Stochastic Gradient Descent (SGD) . With any model, our goal is to minimize our loss in order to improve the accuracy of our models predictions (attempting to avoid overfitting and underfitting of course). The key to improving our models performance is through the opimzation of our loss function. The loss function is indirectly optimized by the modiification of the weights of our model. This optimization is typically done through Gradient Descent. In general, Gradient descent is defined by:\n",
    "\n",
    "$W_{t+1} = W_t - \\alpha \\cdot \\nabla \\text{L}(W_t)$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $W_{t+1}$ is the updated weight\n",
    "\n",
    "- $W$ is the current weight\n",
    "\n",
    "- $\\alpha$ is the learning rate \n",
    "\n",
    "- $\\text{L}(W_t)$ is the gradient vector of the Loss Function, typically denoted J(0).\n",
    "\n",
    "- $t$ is our iteration\n",
    "\n",
    "And of course, this is performed on all weights. The fundamental idea is to take the gradient in order to find out the direction of our \"step\", where that step size is determined my alpha, our learning rate. Stochastic Gradient Descent is this process, on all weights, with a constant alpha. It will be the baseline we will compare all our optimizers to, including Adam.\n",
    "\n",
    "$\\alpha$ in this case, is a hyperparamter, and isn't something optimized by SGD , only chosen by the Machine Learning Scientiest themselves. Learning rate, at its core, is how big of a step we take in the direction of the gradient, to hopefully find a global acceptable loss.\n",
    "# Drawbacks #\n",
    "While 100% a *step* (get it ) in the right direction, a new issue is introduced of attempting to now optimize $\\alpha$. Our steps can now be to0 slow, taking a long time to converge to a minimum, or may be too big, and causing us to never reach the minimum as we shoot out of bounds. Let's tackle each issue one at a time, dealing with the time aspect first. It could be the case that our local minimum is a straight shot from wherever or model is currently at, however, due to an $\\alpha$ being too small, we might take a significant amount of more time than needed. That is where we introduce the concept of **Momentum**.\n",
    "# SGD With Momentum #\n",
    "The best way to introduce the concept of Momentum when it comes to our optimization, is to thing about it exactly as it sounds. We introduce a *Velocity* vector, a vector containing past gradients of our paramters. We now will define SGD with Momentum as:\n",
    "- $V_{t+1} = \\beta*V_{t} + (1-\\beta) \\nabla W_{t}$\n",
    "- $W_{t+1} = W_{t} - \\alpha * V_{t+1}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $V_{t+1}$ is the updated velocity\n",
    "- $W_{t+1}$ is the updated weight\n",
    "- $V_{t}$ is the old velocity\n",
    "- $\\nabla W_{t}$ is the old Gradient of the Loss Function\n",
    "- $W_{t}$ is our old paramter\n",
    "- $\\beta$ is a new hyperparameter, usually kept at around 0.9\n",
    "\n",
    "\n",
    "With this new function, we now add a portion of past gradients to our current gradient before an update occurs. This is how we implement the concept of \"Momentum\", or , \"a big step is made in this direction so let me take a bigger step\". What I found extremley interesing about this defintion of SGD + Momentum, was how buy our definitions, older gradients will naturally be considered less and less as we continue. How So ? Let us consider specifcally the Velocity update expression. If we expand $V_{t}$, we obtain:\n",
    "\n",
    "$V_{t+1} = \\beta*(\\beta*V_{t-1}+(1-\\beta)\\nabla W_{t-1}) + (1-\\beta) \\nabla W_{t}$\n",
    "\n",
    "$V_{t+1} = (\\beta)^2 \\cdot V_{t-1}+\\beta(1-\\beta)\\nabla W_{t-1} + (1-\\beta) \\nabla W_{t}$\n",
    "\n",
    "Since $\\beta$ is a value less than 1, as time goes on, past gradients are multiplied by a smaller consant until they leave little impact. Very Cool !\n",
    "\n",
    "# Drawbacks, again #\n",
    "While a another great *step* forward, we might have to take a *step* back...(I'll stop now). SGD + Momentum boasts another imporvement against SGD, but it isn't perfect. One of the main drawbacks can occur when the gradient of one paramater is significantly larger than the gradient of another paramter. This causes SGD and SGD+Momentum to optimize one parameter first, and then another, taking smaller and smaller steps as it converges. In order to solve this issue, we can propose another optimizer, built upon both of the past, called RMSProp, or Root Mean Squared Propagation.\n",
    "\n",
    "# Root Mean What ? #\n",
    "The goal of RMSProp, is to make sure we equally consider the magnitues of our gradients as we perform our descent. This allows the optimizer to converge faster, and again improves upon the past. We define RMS Prop as:\n",
    "- $V_{t+1} = \\beta*V_{t} + (1-\\beta) \\nabla (W_{t})^2$\n",
    "- $W_{t+1} = W_{t} - \\alpha \\cdot \\frac{\\nabla W_{t}}{\\sqrt{V_{t+1} + \\epsilon}}$\n",
    "\n",
    "The main difference here is where our gradient of the Loss Function is placed. $\\epsilon$ is just a number to prevent division b zero if such a case occured. To clarify again, $V_{t}$ is a vector containing past gradients. The reason why we divide by $\\sqrt{V_{t+1} + \\epsilon}$ is clever:  weights with a history of larger gradients will have a larger denominator when updating the weight, hence a smaller update will be made. This ensures that we balance what weights are optimized while descending. ( $\\nabla W_{t}$ is now squared to prevent negative's)\n",
    "\n",
    "# So Who's Adam ? #\n",
    "Adam, which came from **adaptive moment estimation**, is an optimization algorithm that combines intution derived from both RMSProp and Momentum. Adam has many benifits that include:\n",
    "- Straightforward Implementation\n",
    "- Computational Effeciency\n",
    "- Low memory Requirement\n",
    "\n",
    "The original paper introducing Adam has almost 200,000 citations, and is the default optimizer for most python-based ML libariries. Putting together what we discussed before, we will now wittle down Adam to it's core. We can define Adam as:\n",
    "\n",
    "- $M_{t} = \\beta_{1} \\cdot M_{t-1} + (1-\\beta_{1}) \\nabla W_{t}$\n",
    "- $V_{t} = \\beta_{2} \\cdot V_{t-1} + (1-\\beta_{2}) \\nabla (W_{t})^2$\n",
    "- $\\hat{M_{t}} = \\frac{M_{t}}{1-(\\beta_{1})^t}$\n",
    "- $\\hat{V_{t}} = \\frac{V_{t}}{1-(\\beta_{2})^t}$\n",
    "- $W_{t+1} = W_{t} - \\alpha \\cdot \\frac{\\hat{M_{t}}}{\\sqrt{\\hat{V_{t}}} + \\epsilon}$\n",
    "\n",
    "$\\beta_{1}$ and $\\beta_{2}$ are typically 0.9 and 0.99 respectivley.Again, not to be consusing, but $\\nabla W_{t}$ is the gradient of the loss function of the parameters. Here, we see a few things change in our final weight update, but most of what we see comes from Momentum and RMSProp. We have a new term, $M_{t}$, as well as two new functions. In order to describe this, I'd like to first report on the original paper's abstract. \n",
    "\n",
    "# Two Moments is better than one #\n",
    "\"We introduce Adam, an algorithm for first-order gradient-based optimization of\n",
    "stochastic objective functions, based on adaptive estimates of lower-order mo-\n",
    "ments\"\n",
    "\n",
    "This comes directly from the original white paper, in fact it's the very first sentence of the whole thing. The key to adam is the it's moments, the first being defined by $M$ and the second being defined by $V$. \n",
    "- $M$ (first-moment)represents the mean of the gradients. If the magnitude of the mean is high, that means the gradients along paramaters are pointing approximatley in the same direction.\n",
    "- $V$ (second-moment) represents variance, and comes from RMSProp. It assists in controlling the learning rate among paramters as discussed before.\n",
    "\n",
    "The two new equations are in place to account for the initial bias when training begins. As we continue to train, both $\\beta$'s in the denominator's will eventually reach 1 (as $\\beta$ is < 1 , so it'll approach 0 and leave 1/1), allowing the model to properly train without the intial Bias of what I believe is 0 initiation, \n",
    "\n",
    "By considering these two moment's, building upon RMSProp and Momentum, we obtain an extremley effecient,effective, and cheap optimizer for models. Now that we know exactly where Adam came from, let's compare Adam to the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54032e11-4d5e-42a5-927c-66671be6a764",
   "metadata": {},
   "source": [
    "# Race to Convergence #\n",
    "To show the difference in gradient descent, we will show gradient descent occuruing on a function defined as:\n",
    "\n",
    "def f(x, y):\n",
    "\n",
    "  $  parabola = 0.075 * x**2 + 0.075 * y**2 / 9$\n",
    "   \n",
    "   $ hole1 = -0.5 * (np.exp(-2*(x)**2) * np.exp(-2*(y)**2))$\n",
    "   \n",
    "   $ hole2 = -0.75 * (np.exp(-4*(x-1)**2) * np.exp(-4*(y-1)**2))$\n",
    "\n",
    "    return parabola + hole1 + hole2\n",
    "\n",
    "This an easy to intepret graph, with a local and global minimum. I found this function online, and computed the gradient myself to perform the gradient descent. We will be using a $\\alpha$ =0.05, and will define SGD and Adam. Our result is an interactive graph where we can see the extreme gains Adam boasts over SGD, just by adding a our vectors and including our moments;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1bcf06b-5fd6-4391-8a0c-ad4de31b7091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43b6ec63d4a45f798f1092c5be484ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Steps', max=529, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD converged in 529 steps.\n",
      "Adam converged in 285 steps.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "# Function f(x, y)\n",
    "def f(x, y):\n",
    "    parabola = 0.075 * x**2 + 0.075 * y**2 / 9\n",
    "    hole1 = -0.5 * (np.exp(-2*(x)**2) * np.exp(-2*(y)**2))\n",
    "    hole2 = -0.75 * (np.exp(-4*(x-1)**2) * np.exp(-4*(y-1)**2))\n",
    "    return parabola + hole1 + hole2\n",
    "\n",
    "# Gradient of f(x, y)\n",
    "def gradient_f(x, y):\n",
    "    d_parabola_x = 0.150 * x\n",
    "    d_parabola_y = 0.150 * y / 9\n",
    "    \n",
    "    d_hole1_x = 2 * (x) * np.exp(-2 * (x)**2) * np.exp(-2 * (y)**2)\n",
    "    d_hole1_y = 2 *(y) * np.exp(-2 * (x)**2) * np.exp(-2 * (y)**2)\n",
    "    \n",
    "    d_hole2_x = 6 * (x - 1) * np.exp(-4 * (x - 1)**2) * np.exp(-4 * (y - 1)**2)\n",
    "    d_hole2_y = 6 * (y - 1) * np.exp(-4 * (x - 1)**2) * np.exp(-4 * (y - 1)**2)\n",
    "    \n",
    "    grad_x = d_parabola_x + d_hole1_x + d_hole2_x\n",
    "    grad_y = d_parabola_y + d_hole1_y + d_hole2_y\n",
    "    \n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "# Gradient descent functions\n",
    "\n",
    "# SGD\n",
    "def sgd(x_init, y_init, lr=0.05, epochs=2000, tol=1e-6):\n",
    "    x, y = x_init, y_init\n",
    "    trajectory = [(x, y)]\n",
    "    for i in range(epochs):\n",
    "        grad = gradient_f(x, y)\n",
    "        x -= lr * grad[0]\n",
    "        y -= lr * grad[1]\n",
    "        trajectory.append((x, y))\n",
    "        if np.linalg.norm(grad) < tol:  # Check for convergence\n",
    "            break\n",
    "    return np.array(trajectory)\n",
    "\n",
    "# Adam\n",
    "def adam(x_init, y_init, lr=0.05, epochs=1000, beta1=0.9, beta2=0.99, epsilon=1e-8, tol=1e-6):\n",
    "    x, y = x_init, y_init\n",
    "    m_x, m_y = 0, 0\n",
    "    v_x, v_y = 0, 0\n",
    "    trajectory = [(x, y)]\n",
    "    for t in range(1, epochs + 1):\n",
    "        grad = gradient_f(x, y)\n",
    "        m_x = beta1 * m_x + (1 - beta1) * grad[0]\n",
    "        m_y = beta1 * m_y + (1 - beta1) * grad[1]\n",
    "        v_x = beta2 * v_x + (1 - beta2) * grad[0]**2\n",
    "        v_y = beta2 * v_y + (1 - beta2) * grad[1]**2\n",
    "\n",
    "        m_x_hat = m_x / (1 - beta1**t)\n",
    "        m_y_hat = m_y / (1 - beta1**t)\n",
    "        v_x_hat = v_x / (1 - beta2**t)\n",
    "        v_y_hat = v_y / (1 - beta2**t)\n",
    "\n",
    "        x -= lr * m_x_hat / (np.sqrt(v_x_hat) + epsilon)\n",
    "        y -= lr * m_y_hat / (np.sqrt(v_y_hat) + epsilon)\n",
    "        trajectory.append((x, y))\n",
    "        if np.linalg.norm(grad) < tol:  # Check for convergence\n",
    "            break\n",
    "    return np.array(trajectory)\n",
    "\n",
    "# Initial point\n",
    "x_init, y_init = -2, -2\n",
    "\n",
    "# Run optimizers\n",
    "sgd_path = sgd(x_init, y_init)\n",
    "adam_path = adam(x_init, y_init)\n",
    "\n",
    "# Create meshgrid for surface\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = np.linspace(-2, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Function to plot the optimizer paths up to the current step\n",
    "def plot_with_steps(step):\n",
    "    # Limit the trajectory to the current step\n",
    "    sgd_partial_path = sgd_path[:step]\n",
    "    adam_partial_path = adam_path[:step]\n",
    "\n",
    "    # Plot surface and paths\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "\n",
    "    # Plot optimizer paths up to the current step\n",
    "    ax.plot(sgd_partial_path[:, 0], sgd_partial_path[:, 1], f(sgd_partial_path[:, 0], sgd_partial_path[:, 1]), color='red', label='SGD')\n",
    "    ax.plot(adam_partial_path[:, 0], adam_partial_path[:, 1], f(adam_partial_path[:, 0], adam_partial_path[:, 1]), color='orange', label='Adam')\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_title(f'Gradient Descent Paths at Step {step}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z (Loss)')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    " \n",
    "\n",
    "# Create interactive slider for steps\n",
    "interact(plot_with_steps, step=widgets.IntSlider(value=1, min=1, max=max(len(sgd_path), len(adam_path)), step=1, description='Steps'))\n",
    "# Print how many steps it took to converge at the last step\n",
    "print(f\"SGD converged in {len(sgd_path)} steps.\")\n",
    "print(f\"Adam converged in {len(adam_path)} steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895af44-a7da-4254-8cf6-47618b4d468d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
