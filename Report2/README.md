# Biweekly Report 2: Adam vs The Rest #
In this biweekly report, I studied up and researched "Adam", an optimization technique for gradient descent, so popular it has become the default for many python libraries, even having 200,000 citations in its original paper. Needless to say, Adam provides insight into the importance of mathematical concepts in machine learning, and familiarizing myself with Adam has made me more confident in my ability to parse these concepts. 

( I included a picture of the computation for the gradient in reference to my selected function )


## BiweeklyReport2.ipynb

[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/2-yfGc3a)
